{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nUser ask a question and we take the folowing steps:\\n\\n1. We decide if the questions fall into any of these categories:\\n    -  Summarization: eg. Tell me about Daniel\\n    -  Comparison: eg who has a better job btw Daniel and James\\n    -  Basic_Query: eg how old is daniel\\n    -  General_Knowledge/message: eg Hy, Hello (question that dont require featching context)\\n2. We draw out a plan to answer the question based on the \\n    For instance, lets take a comparison problem\\n    question =  who has a better job btw Daniel and James\\n    steps:\\n     - context_search(what does daniel do?)\\n     - context_search(what does james do?)\\n     - compare results\\n     - draft your final response\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "User ask a question and we take the folowing steps:\n",
    "\n",
    "1. We decide if the questions fall into any of these categories:\n",
    "    -  Summarization: eg. Tell me about Daniel\n",
    "    -  Comparison: eg who has a better job btw Daniel and James\n",
    "    -  Basic_Query: eg how old is daniel\n",
    "    -  General_Knowledge/message: eg Hy, Hello (question that dont require featching context)\n",
    "2. We draw out a plan to answer the question based on the \n",
    "    For instance, lets take a comparison problem\n",
    "    question =  who has a better job btw Daniel and James\n",
    "    steps:\n",
    "     - context_search(what does daniel do?)\n",
    "     - context_search(what does james do?)\n",
    "     - compare results\n",
    "     - draft your final response\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import (HumanMessage,SystemMessage, FunctionMessage)\n",
    "from langchain_core.tools import tool\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "import json\n",
    "\n",
    "default_ef = embedding_functions.DefaultEmbeddingFunction()\n",
    "\n",
    "client = chromadb.PersistentClient(path=\"contexts\")\n",
    "db = client.delete_collection(\"daniel_and_james\")\n",
    "db = client.create_collection(\"daniel_and_james\", embedding_function=default_ef)\n",
    "\n",
    "\n",
    "\n",
    "context_db_for_dan = [\n",
    "    \"He is 45 years old\",\n",
    "    \"Daniel works in tech\",\n",
    "    \"Daniel is a developer\"\n",
    "]\n",
    "context_db_for_james= [\n",
    "    \"He is 49 years old\",\n",
    "    \"James works in finance\",\n",
    "    \"James is a Bank  Manager\"\n",
    "]\n",
    "\n",
    "db.add(documents=context_db_for_dan, ids=['0','1','2'], metadatas=[{\"name\":\"daniel\"}]*3)\n",
    "db.add(documents=context_db_for_james, ids=['3','4','5'], metadatas=[{\"name\":\"james\"}]*3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['3', '0']],\n",
       " 'distances': [[1.0891575047416255, 1.227068588394885]],\n",
       " 'metadatas': [[{'name': 'james'}, {'name': 'daniel'}]],\n",
       " 'embeddings': None,\n",
       " 'documents': [['He is 49 years old', 'He is 45 years old']],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.query(query_texts=[\"age\"], n_results=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['3', '5']],\n",
       " 'distances': [[1.0891575047416255, 1.920391985364417]],\n",
       " 'metadatas': [[{'name': 'james'}, {'name': 'james'}]],\n",
       " 'embeddings': None,\n",
       " 'documents': [['He is 49 years old', 'James is a Bank  Manager']],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filters = {\"name\": {\"$eq\": \"james\"}}\n",
    "db.query(query_texts=[\"age\"], n_results=2, where=filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_search(question, filter_by_name, k=2):\n",
    "    filters = {\"name\": {\"$eq\": filter_by_name.lower()}}\n",
    "    context = db.query(query_texts=question, n_results=k, where=filters )\n",
    "    return context[\"documents\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AGENTS GRAPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated,  Sequence,  TypedDict\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import TypedDict, Annotated \n",
    "from langchain_core.messages import BaseMessage\n",
    "import operator\n",
    "from langgraph.prebuilt.tool_executor import ToolExecutor\n",
    "from langgraph.graph import END, StateGraph\n",
    "import json\n",
    "from langchain_core.messages import (\n",
    "    BaseMessage,\n",
    "    FunctionMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    "    \n",
    ")\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.prebuilt.tool_executor import ToolExecutor, ToolInvocation\n",
    "from langchain_core.tools import tool\n",
    "from typing import Annotated\n",
    "from rich.markdown import Markdown\n",
    "from rich import print as md\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define graph state\n",
    "class AgentState(TypedDict):\n",
    "    chat_history: list[BaseMessage]\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#context_generator agent\n",
    "@tool\n",
    "def query_person_info( keywords : str, filter_by_name: str, k: int = 2) -> list:\n",
    "    \"\"\"\n",
    "    This function searches the database for context related to a given question and a name of person to filter by.\n",
    "    \n",
    "    Parameters:\n",
    "    keywords (str): The keywords to search context for e.g age, occupation etc.\n",
    "    filter_by_name (str): The name of person to filter the search by.\n",
    "    \n",
    "\n",
    "    Returns:\n",
    "    list: A list of documents from the database that match the search criteria.\n",
    "    \"\"\"\n",
    "    filters = {\"name\": {\"$eq\": filter_by_name.lower()}}\n",
    "    context = db.query(query_texts=keywords, n_results=k, where=filters )\n",
    "    return context[\"documents\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "llm2 = ChatOpenAI(model=\"gpt-4-1106-preview\")\n",
    "tools = [query_person_info]\n",
    "functions = [format_tool_to_openai_function(t) for t in tools]\n",
    "context_provider_name = \"context_provider\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "                [\n",
    "                    (\n",
    "                    \"system\",\n",
    "                    \"Your name is {name}, and you are part of an AI system designed to answer questions about some persons\"\n",
    "                    \"The questions are usually about some person named james or daniel \"\n",
    "                    \"Takes this steps:\"\n",
    "                    \"1 . categorise the question into one of the following:\"\n",
    "                    \"A. summarization: This category is for requests that require a summary or info about  of a person's life (just a single person). \"\n",
    "                    \"B. comparison: This category is for requests that require comparing two or more info about two persons \"\n",
    "                    \"C. general_knowledge/message: This category is for general knowledge questions or messages that don't require fetching context about any person. e.g greeting \"\n",
    "                    \"2. if the question falls under A or B, call the {tool} function one or many times to generate the needed contexts to answer the question. use the best search keywords based on the question\"\n",
    "                    \"3. if the questions falls to general_knowledge/message (C), just answer the question without calling the function based on what you know, do not state the category of the question\"\n",
    "                    ),\n",
    "                    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "                ]\n",
    "            )\n",
    "prompt = prompt.partial(name=context_provider_name)\n",
    "prompt = prompt.partial(tool=\", \".join([tool.name for tool in tools]))\n",
    "context_provider_agent=  prompt | llm2.bind_functions(functions)\n",
    "\n",
    "\n",
    "def context_provider_node(state):\n",
    "    result = context_provider_agent.invoke(state)\n",
    "    \n",
    "    if isinstance(result, FunctionMessage):\n",
    "        # Here we can handle the FunctionMessage\n",
    "        if result.function in functions:\n",
    "            # The function/tool is valid\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid function/tool: {result.function}\")\n",
    "    else:\n",
    "        result = AIMessage(**result.dict(exclude={\"type\", \"name\"}), name=context_provider_name)\n",
    "    return {\n",
    "        \"messages\": [result],\n",
    "\n",
    "\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tool Executor\n",
    "tool_executor = ToolExecutor(tools)\n",
    "\n",
    "def tool_node(state):\n",
    "\n",
    "    \"\"\"This runs tools in the graph\n",
    "\n",
    "    It takes in an agent action and calls that tool and returns the result.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "  \n",
    "    # Based on the continue condition\n",
    "    # we know the last message involves a function call\n",
    "    last_message = messages[-1]\n",
    "    # We construct an ToolInvocation from the function_call\n",
    "    try:\n",
    "        tool_input = json.loads(\n",
    "            last_message.additional_kwargs[\"function_call\"][\"arguments\"]\n",
    "        )\n",
    "    except:\n",
    "        tool_input = {\"code\":last_message.additional_kwargs[\"function_call\"][\"arguments\"]} #sometimes the actual code is sent as a string instead of {code:\"code\"}\n",
    "    # We can pass single-arg inputs by value\n",
    "    if len(tool_input) == 1 and \"__arg1\" in tool_input:\n",
    "        tool_input = next(iter(tool_input.values()))\n",
    "        print(tool_input)\n",
    "    tool_name = last_message.additional_kwargs[\"function_call\"][\"name\"]\n",
    "    action = ToolInvocation(\n",
    "        tool=tool_name,\n",
    "        tool_input=tool_input,\n",
    "    )\n",
    "    # We call the tool_executor and get back a response\n",
    "    response = tool_executor.invoke(action)\n",
    "    # We use the response to create a FunctionMessage\n",
    "    function_message = FunctionMessage(\n",
    "        content=f\"{tool_name} response: {str(response)}\", name=action.tool\n",
    "    )\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [function_message]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(context_provider_name, context_provider_node)\n",
    "workflow.add_node(\"call_tool\", tool_node)\n",
    "\n",
    "def router(state):\n",
    "    # This is the router\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    if \"function_call\" in last_message.additional_kwargs:\n",
    "        # The previus agent is invoking a tool\n",
    "        return \"call_tool\" #irrespective of the sender\n",
    "    else:\n",
    "       #proceed to human node\n",
    "       return \"continue\"\n",
    "\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    context_provider_name,\n",
    "    router,\n",
    "    {\n",
    "       \"call_tool\":\"call_tool\", \"continue\":END\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "workflow.add_edge(\n",
    "    \"call_tool\",\n",
    "    context_provider_name,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "workflow.set_entry_point(context_provider_name)\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">I am calling the function <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">query_person_info</span> with the following arguments:                                          \n",
       "{\"keywords\":\"age\",\"filter_by_name\":\"daniel\",\"k\":1}                                                                 \n",
       "</pre>\n"
      ],
      "text/plain": [
       "I am calling the function \u001b[1;36;40mquery_person_info\u001b[0m with the following arguments:                                          \n",
       "{\"keywords\":\"age\",\"filter_by_name\":\"daniel\",\"k\":1}                                                                 \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">I am calling the function <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">query_person_info</span> with the following arguments:                                          \n",
       "{\"keywords\":\"age\",\"filter_by_name\":\"james\",\"k\":1}                                                                  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "I am calling the function \u001b[1;36;40mquery_person_info\u001b[0m with the following arguments:                                          \n",
       "{\"keywords\":\"age\",\"filter_by_name\":\"james\",\"k\":1}                                                                  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Daniel is 45 years old, and James is 49 years old. Therefore, James is older than Daniel.                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Daniel is 45 years old, and James is 49 years old. Therefore, James is older than Daniel.                          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "state = {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=f'hy, who is older btw daniel and james'\n",
    "            )\n",
    "        ],\n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "for s in graph.stream(state, {\"recursion_limit\": 150}):\n",
    "    #print(s)\n",
    "    agent = list(s.keys())[0]\n",
    "    content = s[agent][\"messages\"][-1].content\n",
    "    \n",
    "    if agent in [ context_provider_name]:\n",
    "        #check if it is trying to call a function/tool\n",
    "        if \"function_call\" in s[agent][\"messages\"][-1].additional_kwargs:\n",
    "            function_being_called = s[agent][\"messages\"][-1].additional_kwargs['function_call']['name']\n",
    "            args = s[agent][\"messages\"][-1].additional_kwargs['function_call']['arguments']\n",
    "            content = f\"I am calling the function `{function_being_called}` with the following arguments: {args}\"\n",
    "            content = Markdown(content)\n",
    "            md(content)\n",
    "        else:\n",
    "            content = Markdown(content)\n",
    "            md(content)\n",
    "    elif agent == \"call_tool\":\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import (HumanMessage,SystemMessage, FunctionMessage)\n",
    "from langchain_core.tools import tool\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "import json\n",
    "\n",
    "default_ef = embedding_functions.DefaultEmbeddingFunction()\n",
    "client = chromadb.PersistentClient(path=\"contexts\")\n",
    "db = client.get_collection(\"daniel\", embedding_function=default_ef)\n",
    "\n",
    "\n",
    "@tool\n",
    "def context_search(question: str):\n",
    "    \"\"\"\n",
    "    This function takes a question as input, queries the database for the context related to the question,\n",
    "    prints the documents found, and returns these documents.\n",
    "    \n",
    "    Args:\n",
    "        question (str): The question for which the context is to be found.\n",
    "        \n",
    "    Returns:\n",
    "        list: The documents related to the question.\n",
    "    \"\"\"\n",
    "    context = db.query(query_texts=question, n_results=2,)\n",
    "    print(\"Using \", context['documents'])\n",
    "    return context['documents']\n",
    "\n",
    "\n",
    "tools = [context_search]\n",
    "functions = [format_tool_to_openai_function(t) for t in tools]\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4-1106-preview\")\n",
    "agent = llm.bind_functions(functions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today? If you have any questions or need information, feel free to ask.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "SystemMessage(content=f'''You are an helpful AI Assistant, if user asks questions about Daniel use the context_search function to get context to answer the question'''),\n",
    "HumanMessage(content=\"hy\")            \n",
    "]\n",
    "\n",
    "res = agent.invoke(messages)\n",
    "if res.additional_kwargs:\n",
    "    if \"function_call\" in res.additional_kwargs:\n",
    "        \n",
    "        function_name = res.additional_kwargs[\"function_call\"]['name']\n",
    "        function_args = res.additional_kwargs[\"function_call\"]['arguments']\n",
    "        \n",
    "        print(\"I need to call this function:\", function_name)\n",
    "        print(\"And pass the following args:\", function_args)\n",
    "    \n",
    "        function_result = context_search(json.loads(function_args)[\"question\"])\n",
    "        print(\"Function result:\", function_result)\n",
    "        \n",
    "        messages.append(FunctionMessage(content=f\"{function_name} : {function_result}\",name=function_name ))\n",
    "        \n",
    "        final_res = agent.invoke(messages)\n",
    "        print(final_res.content)\n",
    "else:\n",
    "    print(res.content)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PUTTING THINGS TOGETHER\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=f'''You are an helpful AI Assistant, if user asks questions about Daniel use the context_search function to get context to answer the question'''),           \n",
    "    ]\n",
    "\n",
    "def get_response(question):\n",
    "    messages.append(HumanMessage(content=question)) \n",
    "    res = agent.invoke(messages)\n",
    "    if res.additional_kwargs:\n",
    "        if \"function_call\" in res.additional_kwargs:\n",
    "            \n",
    "            function_name = res.additional_kwargs[\"function_call\"]['name']\n",
    "            function_args = res.additional_kwargs[\"function_call\"]['arguments']\n",
    "            \n",
    "            print(\"I need to call this function:\", function_name)\n",
    "            print(\"And pass the following args:\", function_args)\n",
    "        \n",
    "            function_result = context_search(json.loads(function_args)[\"question\"])\n",
    "            print(\"Function result:\", function_result)\n",
    "            \n",
    "            messages.append(FunctionMessage(content=f\"{function_name} : {function_result}\",name=function_name ))\n",
    "            \n",
    "            final_res = agent.invoke(messages)\n",
    "            return final_res.content\n",
    "    else:\n",
    "        return res.content\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: Hello! How can I assist you today? If you have any questions or need information, feel free to ask.\n",
      "I need to call this function: context_search\n",
      "And pass the following args: {\"question\":\"what does Dan do?\"}\n",
      "Using  [['Daniel works in tech', 'Daniel is a developer']]\n",
      "Function result: [['Daniel works in tech', 'Daniel is a developer']]\n",
      "AI: Daniel is a developer and he works in the tech industry.\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    question = input(\"You: \")\n",
    "    if question == \"exit\":\n",
    "        break\n",
    "    answer = get_response(question)\n",
    "    print(f\"AI: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
